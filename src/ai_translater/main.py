"""ä¸»å…¥å£æ¨¡å— - PDF OCRç¿»è¯‘å·¥å…·

æ”¯æŒåˆ†æ­¥æ‰§è¡Œï¼š
1. extract: PDFè½¬å›¾ç‰‡
2. ocr: å›¾ç‰‡OCRè¯†åˆ«
3. translate: ç¿»è¯‘OCRç»“æœ
4. generate: ç”ŸæˆåŒè¯­PDF
5. all: å®Œæ•´æµç¨‹
"""

import argparse
import json
import os
import sys
from enum import Enum
from pathlib import Path
from typing import List, Optional

from dotenv import load_dotenv

from .ocr_engine import OCREngine, PageOCRResult, TextBlock
from .pdf_extractor import PDFExtractor
from .pdf_generator import BilingualContent, PDFGenerator
from .translator import Translator
from .word_generator import WordGenerator


class OutputFormat(str, Enum):
    """è¾“å‡ºæ ¼å¼"""
    DUAL_COLUMN = "dual"  # å·¦å³åŒæ 
    INTERLEAVED = "interleaved"  # ä¸Šä¸‹äº¤æ›¿
    TRANSLATION_ONLY = "translation"  # ä»…è¯‘æ–‡


# ============== æ­¥éª¤1: æå–å›¾ç‰‡ ==============

def cmd_extract(args):
    """æ‰§è¡ŒPDFæå–å›¾ç‰‡"""
    load_dotenv()
    
    input_pdf = Path(args.input)
    output_dir = Path(args.output_dir)
    
    if not args.quiet:
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†: {input_pdf}")
    
    # è§£æé¡µé¢èŒƒå›´
    page_range = parse_page_range(args.pages)
    
    # æå–å›¾ç‰‡
    extractor = PDFExtractor(dpi=args.dpi)
    
    if not args.quiet:
        print("ğŸ–¼ï¸  æ­£åœ¨æå–PDFé¡µé¢...")
    
    images = extractor.extract_pages(input_pdf, output_dir=output_dir, page_range=page_range)
    
    if not args.quiet:
        print(f"âœ… æå–äº† {len(images)} é¡µåˆ° {output_dir}")


# ============== æ­¥éª¤2: OCRè¯†åˆ« ==============

def cmd_ocr(args):
    """æ‰§è¡ŒOCRè¯†åˆ«"""
    load_dotenv()
    
    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not args.quiet:
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_dir}")
    
    # è·å–æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
    image_files = sorted(input_dir.glob("*.png")) + sorted(input_dir.glob("*.jpg"))
    
    if not image_files:
        print("âŒ æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶", file=sys.stderr)
        sys.exit(1)
    
    if not args.quiet:
        print(f"ğŸ” æ­£åœ¨è¿›è¡ŒOCRè¯†åˆ« ({len(image_files)} å¼ å›¾ç‰‡)...")
    
    # OCRè¯†åˆ«
    ocr = OCREngine(lang=args.lang)
    
    for i, img_path in enumerate(image_files):
        if not args.quiet:
            print(f"   è¯†åˆ«ç¬¬ {i + 1}/{len(image_files)} å¼ : {img_path.name}")
        
        result = ocr.recognize(str(img_path), page_num=i)
        
        # ä¿å­˜ç»“æœ
        page_data = {
            "page": i + 1,
            "source_file": img_path.stem,
            "image_file": img_path.name,
            "text_blocks": [
                {
                    "text": block.text,
                    "confidence": block.confidence,
                    "bbox": block.bbox,
                    "position": {
                        "x": block.x,
                        "y": block.y,
                        "width": block.width,
                        "height": block.height,
                    }
                }
                for block in result.text_blocks
            ],
            "full_text": result.full_text,
            "text_block_count": len(result.text_blocks),
        }
        
        output_file = output_dir / f"page_{i + 1:04d}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(page_data, f, ensure_ascii=False, indent=2)
    
    if not args.quiet:
        print(f"âœ… OCRç»“æœå·²ä¿å­˜åˆ° {output_dir}")


# ============== æ­¥éª¤3: ç¿»è¯‘ ==============

# é¡µé¢åˆ†éš”æ ‡è®°
PAGE_SEPARATOR = "\n\n---PAGE_BREAK---\n\n"


def cmd_translate(args):
    """æ‰§è¡Œç¿»è¯‘ - å…¨æ–‡åˆå¹¶ç¿»è¯‘ï¼Œä¿æŒè·¨é¡µå¥å­å®Œæ•´æ€§"""
    load_dotenv()
    
    input_dir = Path(args.input_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    if not args.quiet:
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_dir}")
    
    # è·å–æ‰€æœ‰OCRç»“æœæ–‡ä»¶
    json_files = sorted(input_dir.glob("*.json"))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°OCRç»“æœæ–‡ä»¶", file=sys.stderr)
        sys.exit(1)
    
    # è¯»å–æ‰€æœ‰OCRç»“æœ
    ocr_data_list = []
    page_texts = []
    
    for json_file in json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            ocr_data = json.load(f)
        ocr_data_list.append((json_file, ocr_data))
        page_texts.append(ocr_data.get("full_text", "").strip())
    
    # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
    merged_text = PAGE_SEPARATOR.join(page_texts)
    
    if not merged_text.strip():
        if not args.quiet:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç¿»è¯‘çš„æ–‡æœ¬")
        # ä¿å­˜ç©ºç»“æœ
        for json_file, ocr_data in ocr_data_list:
            translation_data = {**ocr_data, "translated_text": ""}
            output_file = output_dir / json_file.name
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(translation_data, f, ensure_ascii=False, indent=2)
        return
    
    # åˆå§‹åŒ–ç¿»è¯‘å™¨
    translator = Translator(
        api_key=args.api_key,
        base_url=args.base_url,
        model=args.model,
    )
    
    if not args.quiet:
        print(f"ğŸŒ æ­£åœ¨ç¿»è¯‘ ({len(json_files)} é¡µï¼Œå…¨æ–‡åˆå¹¶æ¨¡å¼)...")
    
    # å…¨æ–‡ç¿»è¯‘ï¼ˆå¸¦é¡µé¢åˆ†éš”æ ‡è®°ï¼‰
    translated_text = translate_with_page_breaks(
        translator, 
        merged_text, 
        len(json_files),
        verbose=not args.quiet
    )
    
    # æŒ‰æ ‡è®°æ‹†åˆ†ç¿»è¯‘ç»“æœ
    translated_pages = translated_text.split("---PAGE_BREAK---")
    translated_pages = [p.strip() for p in translated_pages]
    
    # ç¡®ä¿é¡µæ•°åŒ¹é…
    while len(translated_pages) < len(json_files):
        translated_pages.append("")
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    for i, (json_file, ocr_data) in enumerate(ocr_data_list):
        translated = translated_pages[i] if i < len(translated_pages) else ""
        
        translation_data = {
            **ocr_data,
            "translated_text": translated,
        }
        
        output_file = output_dir / json_file.name
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(translation_data, f, ensure_ascii=False, indent=2)
    
    if not args.quiet:
        print(f"âœ… ç¿»è¯‘ç»“æœå·²ä¿å­˜åˆ° {output_dir}")


def translate_with_page_breaks(
    translator: Translator,
    text: str,
    page_count: int,
    max_chars_per_batch: int = 8000,
    verbose: bool = True,
) -> str:
    """ç¿»è¯‘å¸¦é¡µé¢åˆ†éš”æ ‡è®°çš„æ–‡æœ¬
    
    Args:
        translator: ç¿»è¯‘å™¨å®ä¾‹
        text: å¸¦PAGE_BREAKæ ‡è®°çš„åˆå¹¶æ–‡æœ¬
        page_count: é¡µæ•°
        max_chars_per_batch: æ¯æ‰¹æœ€å¤§å­—ç¬¦æ•°
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        ç¿»è¯‘åçš„æ–‡æœ¬ï¼ˆä¿ç•™PAGE_BREAKæ ‡è®°ï¼‰
    """
    # æ„å»ºç‰¹æ®Šçš„ç¿»è¯‘æç¤º
    context = f"""This is a document with {page_count} pages. 
Pages are separated by "---PAGE_BREAK---" markers.
IMPORTANT: You must preserve all "---PAGE_BREAK---" markers in your translation exactly as they appear.
Translate the content between markers while keeping the markers intact."""
    
    # å¦‚æœæ–‡æœ¬ä¸å¤ªé•¿ï¼Œç›´æ¥ç¿»è¯‘
    if len(text) <= max_chars_per_batch:
        if verbose:
            print(f"   ç¿»è¯‘å…¨æ–‡ ({len(text)} å­—ç¬¦)...")
        result = translator.translate(text, context=context)
        return result.translated
    
    # æ–‡æœ¬å¤ªé•¿ï¼ŒæŒ‰é¡µé¢åˆ†éš”æ ‡è®°åˆ†æ‰¹ç¿»è¯‘
    if verbose:
        print(f"   æ–‡æœ¬è¾ƒé•¿ ({len(text)} å­—ç¬¦)ï¼Œåˆ†æ‰¹ç¿»è¯‘...")
    
    pages = text.split(PAGE_SEPARATOR)
    translated_pages = []
    
    current_batch = []
    current_length = 0
    
    for i, page in enumerate(pages):
        page_length = len(page) + len(PAGE_SEPARATOR)
        
        # å¦‚æœå½“å‰æ‰¹æ¬¡åŠ ä¸Šè¿™é¡µä¼šè¶…è¿‡é™åˆ¶ï¼Œå…ˆç¿»è¯‘å½“å‰æ‰¹æ¬¡
        if current_length + page_length > max_chars_per_batch and current_batch:
            batch_text = PAGE_SEPARATOR.join(current_batch)
            if verbose:
                print(f"   ç¿»è¯‘æ‰¹æ¬¡ ({len(current_batch)} é¡µ)...")
            result = translator.translate(batch_text, context=context)
            
            # æ‹†åˆ†ç¿»è¯‘ç»“æœ
            batch_translated = result.translated.split("---PAGE_BREAK---")
            translated_pages.extend([p.strip() for p in batch_translated])
            
            current_batch = []
            current_length = 0
        
        current_batch.append(page)
        current_length += page_length
    
    # ç¿»è¯‘æœ€åä¸€æ‰¹
    if current_batch:
        batch_text = PAGE_SEPARATOR.join(current_batch)
        if verbose:
            print(f"   ç¿»è¯‘æ‰¹æ¬¡ ({len(current_batch)} é¡µ)...")
        result = translator.translate(batch_text, context=context)
        
        batch_translated = result.translated.split("---PAGE_BREAK---")
        translated_pages.extend([p.strip() for p in batch_translated])
    
    return "\n\n---PAGE_BREAK---\n\n".join(translated_pages)


# ============== æ­¥éª¤4: ç”Ÿæˆæ–‡æ¡£ ==============

def cmd_generate(args):
    """ç”ŸæˆåŒè¯­æ–‡æ¡£ï¼ˆPDFæˆ–Wordï¼‰"""
    load_dotenv()
    
    input_dir = Path(args.input_dir)
    output_file = Path(args.output)
    
    if not args.quiet:
        print(f"ğŸ“‚ è¾“å…¥ç›®å½•: {input_dir}")
    
    # è·å–æ‰€æœ‰ç¿»è¯‘ç»“æœæ–‡ä»¶
    json_files = sorted(input_dir.glob("*.json"))
    
    if not json_files:
        print("âŒ æœªæ‰¾åˆ°ç¿»è¯‘ç»“æœæ–‡ä»¶", file=sys.stderr)
        sys.exit(1)
    
    # è¯»å–ç¿»è¯‘ç»“æœ
    contents: List[BilingualContent] = []
    
    for json_file in json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        original = data.get("full_text", "")
        translated = data.get("translated_text", "")
        page_num = data.get("page", 1) - 1  # è½¬ä¸º0-based
        
        if original or translated:
            contents.append(BilingualContent(
                original=original,
                translated=translated,
                page_num=page_num,
            ))
    
    # æ ¹æ®æ‰©å±•åé€‰æ‹©è¾“å‡ºæ ¼å¼
    is_word = output_file.suffix.lower() in ['.docx', '.doc']
    output_format = OutputFormat(args.format)
    
    if is_word:
        if not args.quiet:
            print(f"ğŸ“ æ­£åœ¨ç”ŸæˆåŒè¯­Wordæ–‡æ¡£ ({len(contents)} é¡µ)...")
        
        generator = WordGenerator()
        
        if output_format == OutputFormat.DUAL_COLUMN:
            generator.generate_dual_column_docx(contents, output_file, args.title)
        elif output_format == OutputFormat.INTERLEAVED:
            generator.generate_interleaved_docx(contents, output_file, args.title)
        else:
            generator.generate_translation_only_docx(contents, output_file, args.title)
        
        if not args.quiet:
            print(f"âœ… Wordæ–‡æ¡£å·²ç”Ÿæˆ: {output_file}")
    else:
        if not args.quiet:
            print(f"ğŸ“ æ­£åœ¨ç”ŸæˆåŒè¯­PDF ({len(contents)} é¡µ)...")
        
        generator = PDFGenerator()
        
        if output_format == OutputFormat.DUAL_COLUMN:
            generator.generate_dual_column_pdf(contents, output_file, args.title)
        elif output_format == OutputFormat.INTERLEAVED:
            generator.generate_interleaved_pdf(contents, output_file, args.title)
        else:
            generator.generate_translation_only_pdf(contents, output_file, args.title)
        
        if not args.quiet:
            print(f"âœ… PDFå·²ç”Ÿæˆ: {output_file}")


# ============== æ­¥éª¤5: å®Œæ•´æµç¨‹ ==============

def cmd_all(args):
    """æ‰§è¡Œå®Œæ•´æµç¨‹"""
    load_dotenv()
    
    input_pdf = Path(args.input)
    output_pdf = Path(args.output)
    work_dir = Path(args.work_dir) if args.work_dir else output_pdf.parent / f".{output_pdf.stem}_work"
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    images_dir = work_dir / "images"
    ocr_dir = work_dir / "ocr_results"
    translations_dir = work_dir / "translations"
    
    images_dir.mkdir(parents=True, exist_ok=True)
    ocr_dir.mkdir(parents=True, exist_ok=True)
    translations_dir.mkdir(parents=True, exist_ok=True)
    
    page_range = parse_page_range(args.pages)
    verbose = not args.quiet
    
    if verbose:
        print(f"ğŸ“„ æ­£åœ¨å¤„ç†: {input_pdf}")
        print(f"ğŸ“ å·¥ä½œç›®å½•: {work_dir}")
    
    # æ­¥éª¤1: æå–å›¾ç‰‡
    if verbose:
        print("\nğŸ–¼ï¸  [1/4] æ­£åœ¨æå–PDFé¡µé¢...")
    
    extractor = PDFExtractor(dpi=args.dpi)
    images = extractor.extract_pages(input_pdf, output_dir=images_dir, page_range=page_range)
    
    if verbose:
        print(f"   æå–äº† {len(images)} é¡µ")
    
    # æ­¥éª¤2: OCRè¯†åˆ«
    if verbose:
        print("\nğŸ” [2/4] æ­£åœ¨è¿›è¡ŒOCRè¯†åˆ«...")
    
    ocr = OCREngine(lang=args.lang)
    image_files = sorted(images_dir.glob("*.png"))
    
    for i, img_path in enumerate(image_files):
        if verbose:
            print(f"   è¯†åˆ«ç¬¬ {i + 1}/{len(image_files)} é¡µ...")
        
        result = ocr.recognize(str(img_path), page_num=i)
        
        page_data = {
            "page": i + 1,
            "source_file": input_pdf.stem,
            "image_file": img_path.name,
            "text_blocks": [
                {
                    "text": block.text,
                    "confidence": block.confidence,
                    "bbox": block.bbox,
                    "position": {
                        "x": block.x,
                        "y": block.y,
                        "width": block.width,
                        "height": block.height,
                    }
                }
                for block in result.text_blocks
            ],
            "full_text": result.full_text,
            "text_block_count": len(result.text_blocks),
        }
        
        output_file = ocr_dir / f"page_{i + 1:04d}.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(page_data, f, ensure_ascii=False, indent=2)
    
    # æ­¥éª¤3: ç¿»è¯‘ï¼ˆå…¨æ–‡åˆå¹¶æ¨¡å¼ï¼‰
    if verbose:
        print("\nğŸŒ [3/4] æ­£åœ¨ç¿»è¯‘ï¼ˆå…¨æ–‡åˆå¹¶æ¨¡å¼ï¼‰...")
    
    translator = Translator(
        api_key=args.api_key,
        base_url=args.base_url,
        model=args.model,
    )
    
    json_files = sorted(ocr_dir.glob("*.json"))
    
    # è¯»å–æ‰€æœ‰OCRç»“æœ
    ocr_data_list = []
    page_texts = []
    
    for json_file in json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            ocr_data = json.load(f)
        ocr_data_list.append((json_file, ocr_data))
        page_texts.append(ocr_data.get("full_text", "").strip())
    
    # åˆå¹¶æ‰€æœ‰é¡µé¢æ–‡æœ¬
    merged_text = PAGE_SEPARATOR.join(page_texts)
    
    if merged_text.strip():
        # å…¨æ–‡ç¿»è¯‘
        translated_text = translate_with_page_breaks(
            translator, 
            merged_text, 
            len(json_files),
            verbose=verbose
        )
        
        # æŒ‰æ ‡è®°æ‹†åˆ†ç¿»è¯‘ç»“æœ
        translated_pages = translated_text.split("---PAGE_BREAK---")
        translated_pages = [p.strip() for p in translated_pages]
    else:
        translated_pages = [""] * len(json_files)
    
    # ç¡®ä¿é¡µæ•°åŒ¹é…
    while len(translated_pages) < len(json_files):
        translated_pages.append("")
    
    # ä¿å­˜ç¿»è¯‘ç»“æœ
    for i, (json_file, ocr_data) in enumerate(ocr_data_list):
        translated = translated_pages[i] if i < len(translated_pages) else ""
        
        translation_data = {
            **ocr_data,
            "translated_text": translated,
        }
        
        output_file = translations_dir / json_file.name
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(translation_data, f, ensure_ascii=False, indent=2)
    
    # æ­¥éª¤4: ç”Ÿæˆæ–‡æ¡£ï¼ˆPDFæˆ–Wordï¼‰
    is_word = output_pdf.suffix.lower() in ['.docx', '.doc']
    doc_type = "Wordæ–‡æ¡£" if is_word else "PDF"
    
    if verbose:
        print(f"\nğŸ“ [4/4] æ­£åœ¨ç”ŸæˆåŒè¯­{doc_type}...")
    
    contents: List[BilingualContent] = []
    translation_files = sorted(translations_dir.glob("*.json"))
    
    for json_file in translation_files:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        original = data.get("full_text", "")
        translated = data.get("translated_text", "")
        page_num = data.get("page", 1) - 1
        
        if original or translated:
            contents.append(BilingualContent(
                original=original,
                translated=translated,
                page_num=page_num,
            ))
    
    output_format = OutputFormat(args.format)
    
    if is_word:
        generator = WordGenerator()
        if output_format == OutputFormat.DUAL_COLUMN:
            generator.generate_dual_column_docx(contents, output_pdf, args.title)
        elif output_format == OutputFormat.INTERLEAVED:
            generator.generate_interleaved_docx(contents, output_pdf, args.title)
        else:
            generator.generate_translation_only_docx(contents, output_pdf, args.title)
    else:
        generator = PDFGenerator()
        if output_format == OutputFormat.DUAL_COLUMN:
            generator.generate_dual_column_pdf(contents, output_pdf, args.title)
        elif output_format == OutputFormat.INTERLEAVED:
            generator.generate_interleaved_pdf(contents, output_pdf, args.title)
        else:
            generator.generate_translation_only_pdf(contents, output_pdf, args.title)
    
    if verbose:
        print(f"\nâœ… å®Œæˆï¼è¾“å‡ºæ–‡ä»¶: {output_pdf}")
        print(f"   ä¸­é—´æ–‡ä»¶ä¿å­˜åœ¨: {work_dir}")


# ============== è¾…åŠ©å‡½æ•° ==============

def parse_page_range(pages_str: Optional[str]) -> Optional[tuple]:
    """è§£æé¡µé¢èŒƒå›´å­—ç¬¦ä¸²"""
    if not pages_str:
        return None
    
    if "-" in pages_str:
        start, end = pages_str.split("-")
        return (int(start) - 1, int(end))
    else:
        page_num = int(pages_str) - 1
        return (page_num, page_num + 1)


# ============== CLIå…¥å£ ==============

def main():
    """CLIä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="PDF OCRç¿»è¯‘å·¥å…· - ä»æ‰«æPDFä¸­æå–æ–‡å­—å¹¶ç¿»è¯‘",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")
    
    # ---- extract å­å‘½ä»¤ ----
    extract_parser = subparsers.add_parser(
        "extract",
        help="ä»PDFæå–å›¾ç‰‡",
        description="å°†PDFæ¯é¡µè½¬æ¢ä¸ºå›¾ç‰‡æ–‡ä»¶",
    )
    extract_parser.add_argument("input", help="è¾“å…¥PDFæ–‡ä»¶è·¯å¾„")
    extract_parser.add_argument("--output-dir", "-o", required=True, help="è¾“å‡ºå›¾ç‰‡ç›®å½•")
    extract_parser.add_argument("--pages", "-p", help="é¡µé¢èŒƒå›´ï¼Œå¦‚ '1-5' æˆ– '3'")
    extract_parser.add_argument("--dpi", type=int, default=300, help="å›¾ç‰‡DPIï¼Œé»˜è®¤300")
    extract_parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    extract_parser.set_defaults(func=cmd_extract)
    
    # ---- ocr å­å‘½ä»¤ ----
    ocr_parser = subparsers.add_parser(
        "ocr",
        help="å¯¹å›¾ç‰‡è¿›è¡ŒOCRè¯†åˆ«",
        description="è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œè¾“å‡ºJSONæ ¼å¼ç»“æœ",
    )
    ocr_parser.add_argument("--input-dir", "-i", required=True, help="è¾“å…¥å›¾ç‰‡ç›®å½•")
    ocr_parser.add_argument("--output-dir", "-o", required=True, help="è¾“å‡ºOCRç»“æœç›®å½•")
    ocr_parser.add_argument("--lang", default="en", help="è¯†åˆ«è¯­è¨€ï¼Œé»˜è®¤en")
    ocr_parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    ocr_parser.set_defaults(func=cmd_ocr)
    
    # ---- translate å­å‘½ä»¤ ----
    translate_parser = subparsers.add_parser(
        "translate",
        help="ç¿»è¯‘OCRç»“æœ",
        description="è¯»å–OCRç»“æœJSONæ–‡ä»¶ï¼Œä½¿ç”¨AIè¿›è¡Œç¿»è¯‘",
    )
    translate_parser.add_argument("--input-dir", "-i", required=True, help="è¾“å…¥OCRç»“æœç›®å½•")
    translate_parser.add_argument("--output-dir", "-o", required=True, help="è¾“å‡ºç¿»è¯‘ç»“æœç›®å½•")
    translate_parser.add_argument("--api-key", help="OpenAI APIå¯†é’¥")
    translate_parser.add_argument("--base-url", help="OpenAI APIåŸºç¡€URL")
    translate_parser.add_argument("--model", help="ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤gpt-4o-mini")
    translate_parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    translate_parser.set_defaults(func=cmd_translate)
    
    # ---- generate å­å‘½ä»¤ ----
    generate_parser = subparsers.add_parser(
        "generate",
        help="ç”ŸæˆåŒè¯­PDF",
        description="ä»ç¿»è¯‘ç»“æœç”ŸæˆåŒè¯­å¯¹ç…§PDF",
    )
    generate_parser.add_argument("--input-dir", "-i", required=True, help="è¾“å…¥ç¿»è¯‘ç»“æœç›®å½•")
    generate_parser.add_argument("--output", "-o", required=True, help="è¾“å‡ºPDFæ–‡ä»¶è·¯å¾„")
    generate_parser.add_argument("--format", "-f", choices=["dual", "interleaved", "translation"],
                                 default="dual", help="è¾“å‡ºæ ¼å¼")
    generate_parser.add_argument("--title", "-t", help="PDFæ–‡æ¡£æ ‡é¢˜")
    generate_parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    generate_parser.set_defaults(func=cmd_generate)
    
    # ---- all å­å‘½ä»¤ ----
    all_parser = subparsers.add_parser(
        "all",
        help="æ‰§è¡Œå®Œæ•´æµç¨‹",
        description="ä»PDFåˆ°åŒè¯­PDFçš„å®Œæ•´æµç¨‹",
        epilog="""
ç¤ºä¾‹:
  ai-translater all input.pdf output.pdf
  ai-translater all input.pdf output.pdf --format interleaved
  ai-translater all input.pdf output.pdf --pages 1-5
  ai-translater all input.pdf output.pdf --work-dir ./work/
        """,
    )
    all_parser.add_argument("input", help="è¾“å…¥PDFæ–‡ä»¶è·¯å¾„")
    all_parser.add_argument("output", help="è¾“å‡ºPDFæ–‡ä»¶è·¯å¾„")
    all_parser.add_argument("--format", "-f", choices=["dual", "interleaved", "translation"],
                            default="dual", help="è¾“å‡ºæ ¼å¼")
    all_parser.add_argument("--pages", "-p", help="é¡µé¢èŒƒå›´ï¼Œå¦‚ '1-5' æˆ– '3'")
    all_parser.add_argument("--title", "-t", help="PDFæ–‡æ¡£æ ‡é¢˜")
    all_parser.add_argument("--work-dir", "-w", help="å·¥ä½œç›®å½•ï¼Œå­˜æ”¾ä¸­é—´æ–‡ä»¶")
    all_parser.add_argument("--dpi", type=int, default=300, help="å›¾ç‰‡DPIï¼Œé»˜è®¤300")
    all_parser.add_argument("--lang", default="en", help="OCRè¯†åˆ«è¯­è¨€ï¼Œé»˜è®¤en")
    all_parser.add_argument("--api-key", help="OpenAI APIå¯†é’¥")
    all_parser.add_argument("--base-url", help="OpenAI APIåŸºç¡€URL")
    all_parser.add_argument("--model", help="ä½¿ç”¨çš„æ¨¡å‹ï¼Œé»˜è®¤gpt-4o-mini")
    all_parser.add_argument("-q", "--quiet", action="store_true", help="å®‰é™æ¨¡å¼")
    all_parser.set_defaults(func=cmd_all)
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        sys.exit(1)
    
    try:
        args.func(args)
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(f"âŒ é…ç½®é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
